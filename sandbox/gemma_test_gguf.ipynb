{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be6213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_cpp; llama_cpp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd52159",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llama_cpp.Llama(\"../gguf/gemma-3-4b-it-q4_0.gguf\", n_ctx=2048, n_gpu_layers=24, n_threads=20, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489545c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    (\"What is Aerial AI?\",\n",
    "        (\"Aerial AI is a modular reasoning system.\",\n",
    "         \"Aerial AI is a popular Japanese video game.\",\n",
    "         \"No data\")),\n",
    "\n",
    "    (\"Who developed Python?\",\n",
    "        (\"Python was created by Guido van Rossum in 1991.\",\n",
    "         \"Python was developed by Elon Musk.\",\n",
    "         \"No data\")),\n",
    "\n",
    "    (\"What is the capital of France?\",\n",
    "        (\"The capital of France is Paris.\",\n",
    "         \"The capital of France is Rome.\",\n",
    "         \"No data\")),\n",
    "\n",
    "    (\"What does GPU stand for?\",\n",
    "        (\"GPU stands for Graphics Processing Unit.\",\n",
    "         \"GPU stands for General Purpose Utility.\",\n",
    "         \"No data\")),\n",
    "\n",
    "    (\"What is the boiling point of water at sea level?\",\n",
    "        (\"The boiling point of water at sea level is 100°C.\",\n",
    "         \"The boiling point of water at sea level is 50°C.\",\n",
    "         \"No data\")),\n",
    "\n",
    "    (\"Who wrote '1984'?\",\n",
    "        (\"The novel '1984' was written by George Orwell.\",\n",
    "         \"The novel '1984' was written by J. K. Rowling.\",\n",
    "         \"No data\")),\n",
    "\n",
    "    (\"What is 2 + 2?\",\n",
    "        (\"2 + 2 = 4.\",\n",
    "         \"2 + 2 = 5.\",\n",
    "         \"No data\")),\n",
    "\n",
    "    (\"What is the main function of the heart?\",\n",
    "        (\"The main function of the heart is to pump blood through the body.\",\n",
    "         \"The main function of the heart is to digest food.\",\n",
    "         \"No data\")),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc6aad",
   "metadata": {},
   "source": [
    "##### Prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d2606",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question, knowledges in tests:\n",
    "    print(f\"Testing on question: {question}\")\n",
    "    for ext_knowledge in knowledges:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \"content\": \n",
    "                \"You are an AI-interpreter of reference data. \"\n",
    "                \"Use the data provided in the `EXTERNAL KNOWLEDGE` field to respond to the user. \"\n",
    "                \"If the data in this field is recorded as `No data`, simply say that you do not \"\n",
    "                \"know the answer. Do not write anything in your response that is not mentioned in \"\n",
    "                \"the `EXTERNAL KNOWLEDGE` field, even if you can answer the question. \"\n",
    "                \"Try to be as technically accurate.\\n\"\n",
    "                f\"\\nEXTERNAL KNOWLEDGE: {ext_knowledge}.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "        ]\n",
    "\n",
    "        print(\"\\t\", model.create_chat_completion(messages)[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517af987",
   "metadata": {},
   "source": [
    "##### Prompt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question, knowledges in tests:\n",
    "    print(f\"Testing on question: {question}\")\n",
    "    for ext_knowledge in knowledges:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \"content\": \n",
    "                \"Rephrase the information from `EXTERNAL KNOWLEDGE` in a clear, neutral tone. \"\n",
    "                \"Do not introduce new facts, omit details, or change the meaning. \"\n",
    "                \"If `EXTERNAL KNOWLEDGE` value is `No data`, state 'No data provided' without elaboration.\\n\"\n",
    "                f\"\\nEXTERNAL KNOWLEDGE: {ext_knowledge}\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "        ]\n",
    "\n",
    "        print(\"\\t\", model.create_chat_completion(messages)[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51802015",
   "metadata": {},
   "source": [
    "##### Prompt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question, knowledges in tests:\n",
    "    print(f\"Testing on question: {question}\")\n",
    "    for ext_knowledge in knowledges:\n",
    "        messages = [\n",
    "            {   \n",
    "                \"role\": \"system\", \"content\": \n",
    "                \"Respond to the user's question using only the information provided in `EXTERNAL KNOWLEDGE`. \"\n",
    "                \"Do not add any facts, assumptions, or interpretations not explicitly mentioned. \"\n",
    "                \"Rephrase the data in clear, concise language, avoiding robotic or overly technical phrasing. \"\n",
    "                \"If `EXTERNAL KNOWLEDGE` value is `No data`, state 'No data provided' without elaboration.\\n\"\n",
    "                f\"\\nEXTERNAL KNOWLEDGE: {ext_knowledge}.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "        ]\n",
    "\n",
    "        print(\"\\t\", model.create_chat_completion(messages)[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8029c180",
   "metadata": {},
   "source": [
    "### Prompt 4 (final choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question, knowledges in tests:\n",
    "    print(f\"Testing on question: {question}\")\n",
    "    for ext_knowledge in knowledges:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \"content\":\n",
    "                \"Rules:\\n\"\n",
    "                \"1. If EXTERNAL KNOWLEDGE = No data → reply 'I don’t know.'\\n\"\n",
    "                \"2. Otherwise → reply ONLY using the text inside EXTERNAL KNOWLEDGE.\\n\"\n",
    "                \"3. Do not add information from anywhere else.\\n\"\n",
    "                \"4. Be technically precise.\"\n",
    "                \"\\n---\\n\"\n",
    "                f\"EXTERNAL KNOWLEDGE:\\n{ext_knowledge}\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "        ]\n",
    "\n",
    "        print(\"\\t\", model.create_chat_completion(messages)[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111228f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d7ebb",
   "metadata": {},
   "source": [
    "Model with **4b weights** are the best for this task. Models with **<1b** weights can be only _fine-tuned_, so i'm need to collect dataset from the bigger model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
